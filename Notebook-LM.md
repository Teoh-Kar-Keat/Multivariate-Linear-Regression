根據您對「Automobile Dataset（汽車價格資料集）」使用線性迴歸分析的需求，以下將綜合資料集特性、機器學習建模流程中的關鍵步驟與最佳實務進行整理，涵蓋特徵選擇、資料前處理、模型訓練與評估方法，特別聚焦於單變量、多元迴歸，以及 Lasso/Ridge 等正則化迴歸分析的應用。

---

### I. 資料集背景與迴歸任務定義

「Automobile Dataset」是一個用於迴歸任務的**多變量**資料集，共有 205 筆樣本和 25 個特徵，特徵類型包含類別型、整數和實數。

主要任務是**迴歸**，即預測汽車的連續數值結果，通常為**價格 (price)**。此資料集的目標變數 $y$ 為連續數值型態。

**關鍵挑戰點：**

- **缺失值：** 該數據集含有缺失值，特別是在目標變數 $price$ 以及數值特徵如 $normalized-losses$、 $peak-rpm$、 $horsepower$、$stroke$、$bore$ 等。
- **多重共線性：** 線性迴歸對自變數間高度相關性（共線性）敏感，可能影響係數估計的穩定性。

---

### II. 資料前處理技巧與最佳實務

線性迴歸對輸入資料的品質與分佈假設（假設誤差項符合常態分佈）較為敏感，因此需要執行細緻的資料前處理。

### 1. 缺失值處理（Missing Value Handling）

Automobile Dataset 存在多處缺失值，處理方法應根據變數類型和缺失分佈來選擇：

- **刪除法 (Deletion)：** 若缺失比例極低且樣本數充足，可刪除包含缺失值的列或欄位。
- **填補法 (Imputation)：**
    - **中位數填補：** 由於線性迴歸對極端值敏感，使用**中位數**填補（相較於平均數）較不受極端值影響，尤其適用於處理數值型特徵。
    - **預測模型填補：** 可透過迴歸或分類模型預測缺值欄位，適用於特徵間具有高度相關性時。

### 2. 數值特徵轉換與標準化（Numerical Transformation & Scaling）

- **標準化/正規化 (Standardization/Normalization)：**
    - **目的：** 線性模型對輸入資料的尺度敏感。應將數值特徵縮放到相同的範圍，以避免尺度不一致對模型訓練造成偏誤。
    - **方法：** 常見方法為標準化 (Standardization) 或正規化 (Normalization)。
- **處理偏態 (Skewness)：**
    - **檢查：** 透過直方圖或偏態統計量檢查分佈是否對稱。
    - **轉換：** 若特徵（如 $price$ 或其他連續變數）呈現**高度偏態**，應進行**對數轉換 (Log Transform)** 或 Box-Cox 轉換，使其分佈更接近常態分佈。這有助於滿足線性迴歸對誤差常態分佈的假設。

### 3. 類別特徵編碼（Categorical Encoding）

Automobile Dataset 包含多個類別型特徵，例如 $fuel-type$、$body-style$ 等。

- **獨熱編碼 (One-hot Encoding)：** 適用於**無序類別變數**。應將這些類別特徵轉換為數值表示，為每個類別新增一個欄位，該類別為 1，其餘為 0。
- **限制：** 若類別變數具有**高基數 (High Cardinality)**（即類別數量多），One-hot Encoding 會大幅增加特徵維度，造成運算負擔。

---

### III. 特徵選擇方法與共線性處理

特徵選擇旨在從 25 個原始特徵中，篩選出對汽車價格預測最具影響力的子集。

### 1. 特徵選擇方法

- **過濾法 (Filter Method)：**
    - **皮爾森相關係數：** 透過計算特徵與目標變數 ($price$) 之間的相關係數，篩選出最具相關性的特徵。
- **嵌入法 (Embedded Method) - 正則化迴歸：**
    - **Lasso 迴歸 (L1 正則化)：** Lasso 在損失函數中加入權重絕對值的總和（L1 正則化項）。除了限制係數大小外，它最顯著的優點是**能將部分不重要的係數直接縮減為零**，從而自動完成**特徵選擇**。若資料集特徵數量多且希望產生簡化的模型，Lasso 是理想選擇。

### 2. 多重共線性處理（Multicollinearity Handling）

當多個自變數之間高度相關時，可能導致迴歸係數不穩定，影響模型解釋力。

- **嶺迴歸 (Ridge Regression, L2 正則化)：**
    - **定義：** 在損失函數中加入權重平方和（L2 正則化項），防止權重過大。
    - **優勢：** Ridge 迴歸特別適合用於解決**多重共線性**問題。它能穩定係數估計，但**不會將係數壓縮為零**，因此無法自動執行特徵選擇。
- **主成分分析 (PCA)：**
    - PCA 是一種維度簡化技術，可將原始特徵重組，找出能最大化資料變異量的正交向量基底。
    - 在線性迴歸中，PCA 可用於將受多重共線性困擾的變數組合成一個因子（或一組主成分），然後用這些主成分來運行迴歸模型。
- **實務建議：** 在 Automobile 價格預測中，建議同時嘗試 **Lasso**（用於特徵選擇與模型簡化）和 **Ridge**（用於處理共線性和穩定係數），並透過交叉驗證來調整正則化係數 $\lambda$。

---

### IV. 模型訓練與優化策略

線性迴歸模型本質上是透過**最小平方估計 (OLS)** 尋找一條「最佳擬合線」，以最小化預測誤差的平方總和。

### 1. 資料劃分與驗證

- **資料分割：** 將資料集劃分為**訓練集 (Training Set)**、**驗證集 (Validation Set)** 和**測試集 (Test Set)**。訓練集用於參數學習，驗證集用於超參數調校（例如 Lasso/Ridge 的 $\lambda$ 值）和監控過擬合，測試集則用於最終評估。
- **交叉驗證 (K-fold Cross-Validation)：** 尤其當資料量有限時，建議使用 K-fold 交叉驗證來更穩健地評估模型的泛化能力。

### 2. 損失函數與優化器

- **損失函數 (Loss Function)：** 對於汽車價格預測這類迴歸任務，標準的損失函數是**均方誤差 (Mean Squared Error, MSE)**。MSE 藉由懲罰預測值與實際值的平方差，引導模型輸出接近真實的連續數值。
    - 若數據包含較多**異常值**，也可以考慮使用**平均絕對誤差 (MAE)**，因為 MAE 對極端值較不敏感，或採用 Huber 損失以取得平衡。
- **優化器 (Optimizer)：** 對於線性模型，OLS 可直接求解。但如果特徵多或使用正則化，可能需採用梯度下降法：
    - **Mini-batch SGD：** 兼顧訓練穩定性和效率，是常見的優化策略。
    - **Adam：** 是一種進階優化器，因其快速且穩定，常作為深度學習和複雜模型（如正則化迴歸）訓練的萬用優化器。

### 3. 模型穩定化與正則化

- **防止過擬合：** 線性迴歸模型在特徵數多時仍可能過擬合，特別是當引入多項式特徵時。
- **Lasso/Ridge 的應用：** 這兩種正則化方法就是透過引入懲罰項 $\lambda$ (正則化係數) 來限制模型複雜度，藉此避免過度擬合。 $\lambda$ 值的選擇需要通過交叉驗證來決定。

---

### V. 模型評估指標與方法

迴歸任務的模型評估重點在於衡量預測值與實際值之間的偏差程度。

| 評估指標 | 定義與說明 | 最佳實務考量 |
| --- | --- | --- |
| **均方誤差 (MSE)** | 測量預測值與實際值之間誤差的平方平均，數值越小預測越準確。 | 線性迴歸的預設損失函數，對大誤差懲罰較重。 |
| **均方根誤差 (RMSE)** | MSE 的平方根，單位與目標變數 $price$ 相同，易於解讀。 | 用於衡量預測值與真實值之間的**絕對偏差**，單位化後便於理解誤差幅度。 |
| **平均絕對誤差 (MAE)** | 預測值與實際值之間誤差的絕對值平均。 | 對於 $price$ 數據中的異常值較具**魯棒性**（不敏感）。 |
| **決定係數 ($R^2$)** | 代表模型能解釋目標變數變異的比例，範圍在 $0 \sim 1$。 | 越接近 $1$ 表示模型解釋力越強，是判斷模型整體擬合優劣的標準。 |
| **調整後 $R^2$ (Adjusted $R^2$)** | 考慮了自變數數量對模型複雜度的影響，適用於比較不同複雜度模型。 | 在多元迴歸中，當比較包含不同數量特徵的模型時，應優先使用調整後 $R^2$。 |

綜合運用上述指標，特別是結合 **MAE**（魯棒性）、**RMSE**（誤差規模）和 **$R^2$**（模型解釋力），能對汽車價格預測模型的性能進行全面的評估。
